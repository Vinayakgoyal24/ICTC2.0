{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "from transformers import pipeline, BertForSequenceClassification,BertTokenizerFast\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device= 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ben</th>\n",
       "      <th>guj</th>\n",
       "      <th>hin</th>\n",
       "      <th>kan</th>\n",
       "      <th>mal</th>\n",
       "      <th>ori</th>\n",
       "      <th>pan</th>\n",
       "      <th>tam</th>\n",
       "      <th>tel</th>\n",
       "      <th>urd</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sidny opera houser ashpashe thaka suijarlander...</td>\n",
       "      <td>sydney opera house aaspaasma yojayeli spardham...</td>\n",
       "      <td>sidney opera house kii prishthbhumi main switz...</td>\n",
       "      <td>sydney opera housein avarandalli aayojislagidd...</td>\n",
       "      <td>sydney oppara hasiൻre parisarsliൽwech, switzeർ...</td>\n",
       "      <td>sidni opera houser pariveshtani madhyare anust...</td>\n",
       "      <td>sidney opera house de aale-duale sthit svissza...</td>\n",
       "      <td>sidney opera housein suzlil natant ant vilyatl...</td>\n",
       "      <td>sydney opera house parisarall jarigin i kridal...</td>\n",
       "      <td>sidney opera house ke gurdonvah main kaayam, s...</td>\n",
       "      <td>Set in the surroundings of the Sydney Opera Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>con jivanu upar kaaj karbe sei anuyayi aigulo ...</td>\n",
       "      <td>sukshmajivanuona jooth par karaati asarne aadh...</td>\n",
       "      <td>jin sookshmaanuon ke samooh par ye kaam karti ...</td>\n",
       "      <td>ivugannu avu prenam biruv sookshmajivi gumpige...</td>\n",
       "      <td>badhikkunn sukshmajiviute waർggae atisthanmaki...</td>\n",
       "      <td>yeun bhootaanu goshthigudik upare eha kaam kar...</td>\n",
       "      <td>inhan nuun anti- vistyle, anti- fungel, anti-p...</td>\n",
       "      <td>ews ent vaka nunnurigolin midhu vinapurikinden...</td>\n",
       "      <td>ivi prabhavan chupe sukshmajivas samuhaniki an...</td>\n",
       "      <td>inhen in jaraasam ke group ke lihaaz se jin pa...</td>\n",
       "      <td>These are classified as anti-virals, anti-fung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bharter drut arth lenden vyavastha- upiai-ke a...</td>\n",
       "      <td>bharat bhugatan vyavasthao anaya adhikarakshet...</td>\n",
       "      <td>bharat kii sabase twarit bhugtan pranali - upi...</td>\n",
       "      <td>bharatad vegavad pavati vyavastheyad you.pi.i....</td>\n",
       "      <td>indiaille pettennu panamadaykanavunn nilevilul...</td>\n",
       "      <td>bharatar drut arth pradaan brivastha - eu.pi.a...</td>\n",
       "      <td>bharat de bhugtan karan wale shaylia da doosar...</td>\n",
       "      <td>indiavin yubis enepatum duridab panamseluttal ...</td>\n",
       "      <td>saguthunn bharat shighra chellimp vidhana - yu...</td>\n",
       "      <td>bhaarat ke adaegi ke nizaam ko digar dayera ak...</td>\n",
       "      <td>The possibility of linking India's payment sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fasal-poorvavarti evan fasal-paravarty caryakr...</td>\n",
       "      <td>paak poorveni ane paak pachhini pravrittio mat...</td>\n",
       "      <td>fasal kii kataai se pehle or uske baad die jan...</td>\n",
       "      <td>coylige munchin hagu coylen nantarad chatuvati...</td>\n",
       "      <td>villvetuppinumumpu villvetuppinusheavumulla pr...</td>\n",
       "      <td>amal-poorvavartti tatha amal-paravartti karyan...</td>\n",
       "      <td>fasal di kataai ton pihala ate bood vich us te...</td>\n",
       "      <td>aruvadiku mundaiy manllum bindaiy seyalbadugal...</td>\n",
       "      <td>pantakotku mundu, taruvati panul koraku ichche...</td>\n",
       "      <td>kataai se kabel or kataai ke baad kii sargharm...</td>\n",
       "      <td>Loans for pre-harvest and post-harvest activit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maldwar parikshar paddhati ekati drut paddhati...</td>\n",
       "      <td>gudamarg deetha thati tapas ek jhadpi padhdhat...</td>\n",
       "      <td>malashay-sambandhi pareekshan ek twarit vidhi ...</td>\n",
       "      <td>antah-gudnal pariksheyu kshipravad vidhanvagid...</td>\n",
       "      <td>computaർ samvidhana upyogitchull gudaparishodh...</td>\n",
       "      <td>malashai pariksha eka trit paddhati ate (jane ...</td>\n",
       "      <td>andakosh sambandi nirikhan ik jaladi on vaali ...</td>\n",
       "      <td>asana malakutal parichodanai woru viraivana va...</td>\n",
       "      <td>purishanalam dwara pariksh ock satvar vidhana ...</td>\n",
       "      <td>par-ractil maan ek tej tareeq car hai (ek taju...</td>\n",
       "      <td>The per-rectal examination is a quick method (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ben  \\\n",
       "0  sidny opera houser ashpashe thaka suijarlander...   \n",
       "1  con jivanu upar kaaj karbe sei anuyayi aigulo ...   \n",
       "2  bharter drut arth lenden vyavastha- upiai-ke a...   \n",
       "3  fasal-poorvavarti evan fasal-paravarty caryakr...   \n",
       "4  maldwar parikshar paddhati ekati drut paddhati...   \n",
       "\n",
       "                                                 guj  \\\n",
       "0  sydney opera house aaspaasma yojayeli spardham...   \n",
       "1  sukshmajivanuona jooth par karaati asarne aadh...   \n",
       "2  bharat bhugatan vyavasthao anaya adhikarakshet...   \n",
       "3  paak poorveni ane paak pachhini pravrittio mat...   \n",
       "4  gudamarg deetha thati tapas ek jhadpi padhdhat...   \n",
       "\n",
       "                                                 hin  \\\n",
       "0  sidney opera house kii prishthbhumi main switz...   \n",
       "1  jin sookshmaanuon ke samooh par ye kaam karti ...   \n",
       "2  bharat kii sabase twarit bhugtan pranali - upi...   \n",
       "3  fasal kii kataai se pehle or uske baad die jan...   \n",
       "4  malashay-sambandhi pareekshan ek twarit vidhi ...   \n",
       "\n",
       "                                                 kan  \\\n",
       "0  sydney opera housein avarandalli aayojislagidd...   \n",
       "1  ivugannu avu prenam biruv sookshmajivi gumpige...   \n",
       "2  bharatad vegavad pavati vyavastheyad you.pi.i....   \n",
       "3  coylige munchin hagu coylen nantarad chatuvati...   \n",
       "4  antah-gudnal pariksheyu kshipravad vidhanvagid...   \n",
       "\n",
       "                                                 mal  \\\n",
       "0  sydney oppara hasiൻre parisarsliൽwech, switzeർ...   \n",
       "1  badhikkunn sukshmajiviute waർggae atisthanmaki...   \n",
       "2  indiaille pettennu panamadaykanavunn nilevilul...   \n",
       "3  villvetuppinumumpu villvetuppinusheavumulla pr...   \n",
       "4  computaർ samvidhana upyogitchull gudaparishodh...   \n",
       "\n",
       "                                                 ori  \\\n",
       "0  sidni opera houser pariveshtani madhyare anust...   \n",
       "1  yeun bhootaanu goshthigudik upare eha kaam kar...   \n",
       "2  bharatar drut arth pradaan brivastha - eu.pi.a...   \n",
       "3  amal-poorvavartti tatha amal-paravartti karyan...   \n",
       "4  malashai pariksha eka trit paddhati ate (jane ...   \n",
       "\n",
       "                                                 pan  \\\n",
       "0  sidney opera house de aale-duale sthit svissza...   \n",
       "1  inhan nuun anti- vistyle, anti- fungel, anti-p...   \n",
       "2  bharat de bhugtan karan wale shaylia da doosar...   \n",
       "3  fasal di kataai ton pihala ate bood vich us te...   \n",
       "4  andakosh sambandi nirikhan ik jaladi on vaali ...   \n",
       "\n",
       "                                                 tam  \\\n",
       "0  sidney opera housein suzlil natant ant vilyatl...   \n",
       "1  ews ent vaka nunnurigolin midhu vinapurikinden...   \n",
       "2  indiavin yubis enepatum duridab panamseluttal ...   \n",
       "3  aruvadiku mundaiy manllum bindaiy seyalbadugal...   \n",
       "4  asana malakutal parichodanai woru viraivana va...   \n",
       "\n",
       "                                                 tel  \\\n",
       "0  sydney opera house parisarall jarigin i kridal...   \n",
       "1  ivi prabhavan chupe sukshmajivas samuhaniki an...   \n",
       "2  saguthunn bharat shighra chellimp vidhana - yu...   \n",
       "3  pantakotku mundu, taruvati panul koraku ichche...   \n",
       "4  purishanalam dwara pariksh ock satvar vidhana ...   \n",
       "\n",
       "                                                 urd  \\\n",
       "0  sidney opera house ke gurdonvah main kaayam, s...   \n",
       "1  inhen in jaraasam ke group ke lihaaz se jin pa...   \n",
       "2  bhaarat ke adaegi ke nizaam ko digar dayera ak...   \n",
       "3  kataai se kabel or kataai ke baad kii sargharm...   \n",
       "4  par-ractil maan ek tej tareeq car hai (ek taju...   \n",
       "\n",
       "                                                 eng  \n",
       "0  Set in the surroundings of the Sydney Opera Ho...  \n",
       "1  These are classified as anti-virals, anti-fung...  \n",
       "2  The possibility of linking India's payment sys...  \n",
       "3  Loans for pre-harvest and post-harvest activit...  \n",
       "4  The per-rectal examination is a quick method (...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(os.path.join(\"D:\\ICTC2.0\",\"train.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2id={}\n",
    "id2l={}\n",
    "for i,col in enumerate(df.columns):\n",
    "    l2id[col]=i\n",
    "    id2l[i]=col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"dbmdz/bert-base-turkish-uncased\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"dbmdz/bert-base-turkish-uncased\", num_labels=11, id2label=id2l, label2id=l2id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size=df.shape\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_18408\\1701897921.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reshaped_data=reshaped_data.append(tdf,ignore_index=True)\n",
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_18408\\1701897921.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reshaped_data=reshaped_data.append(tdf,ignore_index=True)\n",
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_18408\\1701897921.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reshaped_data=reshaped_data.append(tdf,ignore_index=True)\n",
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_18408\\1701897921.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reshaped_data=reshaped_data.append(tdf,ignore_index=True)\n",
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_18408\\1701897921.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reshaped_data=reshaped_data.append(tdf,ignore_index=True)\n",
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_18408\\1701897921.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reshaped_data=reshaped_data.append(tdf,ignore_index=True)\n",
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_18408\\1701897921.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reshaped_data=reshaped_data.append(tdf,ignore_index=True)\n",
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_18408\\1701897921.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reshaped_data=reshaped_data.append(tdf,ignore_index=True)\n",
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_18408\\1701897921.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reshaped_data=reshaped_data.append(tdf,ignore_index=True)\n",
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_18408\\1701897921.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reshaped_data=reshaped_data.append(tdf,ignore_index=True)\n",
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_18408\\1701897921.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  reshaped_data=reshaped_data.append(tdf,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "reshaped_data=pd.DataFrame(columns=['text','lang'])\n",
    "for col in df.columns:\n",
    "    values=df[col].tolist()\n",
    "    tdf=pd.DataFrame({'text':values,'lang':[col]*len(values)})\n",
    "    reshaped_data=reshaped_data.append(tdf,ignore_index=True)\n",
    "\n",
    "reshaped_data.to_csv(\"reshaped_file.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sidny opera houser ashpashe thaka suijarlander...</td>\n",
       "      <td>ben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>con jivanu upar kaaj karbe sei anuyayi aigulo ...</td>\n",
       "      <td>ben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bharter drut arth lenden vyavastha- upiai-ke a...</td>\n",
       "      <td>ben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fasal-poorvavarti evan fasal-paravarty caryakr...</td>\n",
       "      <td>ben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maldwar parikshar paddhati ekati drut paddhati...</td>\n",
       "      <td>ben</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text lang\n",
       "0  sidny opera houser ashpashe thaka suijarlander...  ben\n",
       "1  con jivanu upar kaaj karbe sei anuyayi aigulo ...  ben\n",
       "2  bharter drut arth lenden vyavastha- upiai-ke a...  ben\n",
       "3  fasal-poorvavarti evan fasal-paravarty caryakr...  ben\n",
       "4  maldwar parikshar paddhati ekati drut paddhati...  ben"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_org=pd.read_csv(os.path.join(\"D:\\ICTC2.0\",os.path.join(\"text_classification\",\"reshaped_file.csv\")))\n",
    "df_org.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sidny opera houser ashpashe thaka suijarlander...</td>\n",
       "      <td>ben</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>con jivanu upar kaaj karbe sei anuyayi aigulo ...</td>\n",
       "      <td>ben</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bharter drut arth lenden vyavastha- upiai-ke a...</td>\n",
       "      <td>ben</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fasal-poorvavarti evan fasal-paravarty caryakr...</td>\n",
       "      <td>ben</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maldwar parikshar paddhati ekati drut paddhati...</td>\n",
       "      <td>ben</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18685</th>\n",
       "      <td>How can we buy that?</td>\n",
       "      <td>eng</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18686</th>\n",
       "      <td>It is made of chickpea flour and other herbs.</td>\n",
       "      <td>eng</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18687</th>\n",
       "      <td>An international cargo hub project, the Multi-...</td>\n",
       "      <td>eng</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18688</th>\n",
       "      <td>Haha yeah!</td>\n",
       "      <td>eng</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18689</th>\n",
       "      <td>This really helps.</td>\n",
       "      <td>eng</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18690 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang  label\n",
       "0      sidny opera houser ashpashe thaka suijarlander...  ben      0\n",
       "1      con jivanu upar kaaj karbe sei anuyayi aigulo ...  ben      0\n",
       "2      bharter drut arth lenden vyavastha- upiai-ke a...  ben      0\n",
       "3      fasal-poorvavarti evan fasal-paravarty caryakr...  ben      0\n",
       "4      maldwar parikshar paddhati ekati drut paddhati...  ben      0\n",
       "...                                                  ...  ...    ...\n",
       "18685                               How can we buy that?  eng     10\n",
       "18686      It is made of chickpea flour and other herbs.  eng     10\n",
       "18687  An international cargo hub project, the Multi-...  eng     10\n",
       "18688                                         Haha yeah!  eng     10\n",
       "18689                                 This really helps.  eng     10\n",
       "\n",
       "[18690 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_org['label']=df_org.lang.map(lambda x: l2id[x.strip()])\n",
    "df_org.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x=df_org['text']\n",
    "y=df_org['label']\n",
    "X_train_temp, X_val_test, y_train_temp, y_val_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12446                 hai devanga. naan nanjhak irukiheen.\n",
       "11983                          its edaykak nataddukhirars?\n",
       "12303    ind varusham rhodes thirumbhi vantirupatal itu...\n",
       "11490     waah! tusin minuun irashaas vich paa ditaa hai I\n",
       "10588    jiadatar ikakia vich tima banai gaia han jo ki...\n",
       "                               ...                        \n",
       "11284    bharat vich, beghatone cup ate aagaa khan tour...\n",
       "11964             tabilufilyui netwarkil live stream achu.\n",
       "5390                     mundin sal nodekke prayatnistini.\n",
       "860      erapar amader royas fo bo evan arevati paschim...\n",
       "15795    ye musanfin kii samaji zimm dari bin gai ki wo...\n",
       "Name: text, Length: 14960, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text=list(X_train_temp)\n",
    "val_text=list(X_val)\n",
    "test_text=list(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_text, truncation=True, padding=True)\n",
    "val_encodings  = tokenizer(val_text, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_text, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for handling tokenized text data and corresponding labels.\n",
    "    Inherits from torch.utils.data.Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Initializes the DataLoader class with encodings and labels.\n",
    "\n",
    "        Args:\n",
    "            encodings (dict): A dictionary containing tokenized input text data\n",
    "                              (e.g., 'input_ids', 'token_type_ids', 'attention_mask').\n",
    "            labels (list): A list of integer labels for the input text data.\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing tokenized data and the corresponding label for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the data item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            item (dict): A dictionary containing the tokenized data and the corresponding label.\n",
    "        \"\"\"\n",
    "        # Retrieve tokenized data for the given index\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Add the label for the given index to the item dictionary\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of data items in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            (int): The number of data items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=list(y_train_temp)\n",
    "test_label=list(y_test)\n",
    "val_label=list(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_encodings, train_label)\n",
    "\n",
    "val_dataloader = DataLoader(val_encodings, val_label)\n",
    "\n",
    "test_dataset = DataLoader(test_encodings, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Computes accuracy, F1, precision, and recall for a given set of predictions.\n",
    "    \n",
    "    Args:\n",
    "        pred (obj): An object containing label_ids and predictions attributes.\n",
    "            - label_ids (array-like): A 1D array of true class labels.\n",
    "            - predictions (array-like): A 2D array where each row represents\n",
    "              an observation, and each column represents the probability of \n",
    "              that observation belonging to a certain class.\n",
    "              \n",
    "    Returns:\n",
    "        dict: A dictionary containing the following metrics:\n",
    "            - Accuracy (float): The proportion of correctly classified instances.\n",
    "            - F1 (float): The macro F1 score, which is the harmonic mean of precision\n",
    "              and recall. Macro averaging calculates the metric independently for\n",
    "              each class and then takes the average.\n",
    "            - Precision (float): The macro precision, which is the number of true\n",
    "              positives divided by the sum of true positives and false positives.\n",
    "            - Recall (float): The macro recall, which is the number of true positives\n",
    "              divided by the sum of true positives and false negatives.\n",
    "    \"\"\"\n",
    "    # Extract true labels from the input object\n",
    "    labels = pred.label_ids\n",
    "    \n",
    "    # Obtain predicted class labels by finding the column index with the maximum probability\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Compute macro precision, recall, and F1 score using sklearn's precision_recall_fscore_support function\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    \n",
    "    # Calculate the accuracy score using sklearn's accuracy_score function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Return the computed metrics as a dictionary\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written\n",
    "    output_dir='./TTC4900Model', \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    #  The number of epochs, defaults to 3.0 \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=32,\n",
    "    # Number of steps used for a linear warmup\n",
    "    warmup_steps=100,                \n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',\n",
    "   # TensorBoard log directory                 \n",
    "    logging_dir='./multi-class-logs',            \n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\", \n",
    "    #fp16=True,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    # the pre-trained model that will be fine-tuned \n",
    "    model=model,\n",
    "     # training arguments that we defined above                        \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataloader,         \n",
    "    eval_dataset=val_dataloader,            \n",
    "    compute_metrics= compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinay\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\vinay/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Unable to read ~/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\ICTC2.0\\text_classification\\wandb\\run-20240305_021942-2lz6bxdj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vinayakgoyal2410/huggingface/runs/2lz6bxdj' target=\"_blank\">northern-monkey-1</a></strong> to <a href='https://wandb.ai/vinayakgoyal2410/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vinayakgoyal2410/huggingface' target=\"_blank\">https://wandb.ai/vinayakgoyal2410/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vinayakgoyal2410/huggingface/runs/2lz6bxdj' target=\"_blank\">https://wandb.ai/vinayakgoyal2410/huggingface/runs/2lz6bxdj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84efef4f5454a9c85fe8273763a2e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3453, 'learning_rate': 2.5e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f97fe72a4840c4a9bd7bc78dc05952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinay\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.146183729171753, 'eval_Accuracy': 0.2620320855614973, 'eval_F1': 0.18670891245761193, 'eval_Precision': 0.3574114438271291, 'eval_Recall': 0.2747875369515326, 'eval_runtime': 564.9993, 'eval_samples_per_second': 3.31, 'eval_steps_per_second': 0.104, 'epoch': 0.05}\n",
      "{'loss': 1.7206, 'learning_rate': 5e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56e708a1e1a43aa918251c2eb6d3890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.199613332748413, 'eval_Accuracy': 0.5796791443850268, 'eval_F1': 0.5436695337427762, 'eval_Precision': 0.6261718730285055, 'eval_Recall': 0.5795719435621578, 'eval_runtime': 562.2196, 'eval_samples_per_second': 3.326, 'eval_steps_per_second': 0.105, 'epoch': 0.11}\n",
      "{'loss': 1.1194, 'learning_rate': 4.9075785582255084e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf8f9b8460a42918302ebdbb717ebf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9175289869308472, 'eval_Accuracy': 0.6641711229946524, 'eval_F1': 0.6473512694619606, 'eval_Precision': 0.7168814067013698, 'eval_Recall': 0.6610905693131199, 'eval_runtime': 559.1252, 'eval_samples_per_second': 3.345, 'eval_steps_per_second': 0.106, 'epoch': 0.16}\n",
      "{'loss': 0.855, 'learning_rate': 4.8151571164510165e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca5a7410eb948ffb7aa15a8a4ddb8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6535883545875549, 'eval_Accuracy': 0.7839572192513369, 'eval_F1': 0.778514038000715, 'eval_Precision': 0.812408533575634, 'eval_Recall': 0.7788661134708957, 'eval_runtime': 546.8037, 'eval_samples_per_second': 3.42, 'eval_steps_per_second': 0.108, 'epoch': 0.21}\n",
      "{'loss': 0.6595, 'learning_rate': 4.722735674676525e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb22bd9c9934e0d81489304496f2979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7003633379936218, 'eval_Accuracy': 0.7556149732620321, 'eval_F1': 0.7494677368048994, 'eval_Precision': 0.8033236292864455, 'eval_Recall': 0.7600511728178087, 'eval_runtime': 549.0713, 'eval_samples_per_second': 3.406, 'eval_steps_per_second': 0.107, 'epoch': 0.27}\n",
      "{'loss': 0.5279, 'learning_rate': 4.6303142329020335e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89938d297fd447e585f62ce35d6be9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.40628448128700256, 'eval_Accuracy': 0.8754010695187165, 'eval_F1': 0.8762657584729273, 'eval_Precision': 0.8825247315978206, 'eval_Recall': 0.8753395514055392, 'eval_runtime': 549.6497, 'eval_samples_per_second': 3.402, 'eval_steps_per_second': 0.107, 'epoch': 0.32}\n",
      "{'loss': 0.4767, 'learning_rate': 4.537892791127542e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9d1d6bb93e4f0da42d8d084396e4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32796382904052734, 'eval_Accuracy': 0.8828877005347594, 'eval_F1': 0.8820594019088066, 'eval_Precision': 0.8934756928728768, 'eval_Recall': 0.884294808401028, 'eval_runtime': 549.3315, 'eval_samples_per_second': 3.404, 'eval_steps_per_second': 0.107, 'epoch': 0.37}\n",
      "{'loss': 0.3889, 'learning_rate': 4.4454713493530505e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc896922f42c4702816418ca65f6365c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3433275520801544, 'eval_Accuracy': 0.8796791443850267, 'eval_F1': 0.8791865712967336, 'eval_Precision': 0.8918933768277149, 'eval_Recall': 0.8806164498864998, 'eval_runtime': 549.3603, 'eval_samples_per_second': 3.404, 'eval_steps_per_second': 0.107, 'epoch': 0.43}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=[trainer.evaluate(eval_dataset=df_org) for df_org in [train_dataloader, val_dataloader, test_dataset]]\n",
    "\n",
    "pd.DataFrame(q, index=[\"train\",\"val\",\"test\"]).iloc[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    \"\"\"\n",
    "    Predicts the class label for a given input text\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text for which the class label needs to be predicted.\n",
    "\n",
    "    Returns:\n",
    "        probs (torch.Tensor): Class probabilities for the input text.\n",
    "        pred_label_idx (torch.Tensor): The index of the predicted class label.\n",
    "        pred_label (str): The predicted class label.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text and move tensors to the GPU if available\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Get model output (logits)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    probs = outputs[0].softmax(1)\n",
    "    \"\"\" Explanation outputs: The BERT model returns a tuple containing the output logits (and possibly other elements depending on the model configuration). In this case, the output logits are the first element in the tuple, which is why we access it using outputs[0].\n",
    "\n",
    "    outputs[0]: This is a tensor containing the raw output logits for each class. The shape of the tensor is (batch_size, num_classes) where batch_size is the number of input samples (in this case, 1, as we are predicting for a single input text) and num_classes is the number of target classes.\n",
    "\n",
    "    softmax(1): The softmax function is applied along dimension 1 (the class dimension) to convert the raw logits into class probabilities. Softmax normalizes the logits so that they sum to 1, making them interpretable as probabilities. \"\"\"\n",
    "\n",
    "    # Get the index of the class with the highest probability\n",
    "    # argmax() finds the index of the maximum value in the tensor along a specified dimension.\n",
    "    # By default, if no dimension is specified, it returns the index of the maximum value in the flattened tensor.\n",
    "    pred_label_idx = probs.argmax()\n",
    "\n",
    "    # Now map the predicted class index to the actual class label \n",
    "    # Since pred_label_idx is a tensor containing a single value (the predicted class index), \n",
    "    # the .item() method is used to extract the value as a scalar\n",
    "    pred_label = model.config.id2label[pred_label_idx.item()]\n",
    "\n",
    "    return probs, pred_label_idx, pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a an example text in Turkish\n",
    "text = \"Makine öğrenimi kendisi de daha da otomatik hale doğru ilerliyor.\"\n",
    "# \"Machine Learning itself is moving towards more and more automated\"\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"text-classification-model\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
